{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T17:23:38.716598400Z",
     "start_time": "2023-09-05T17:23:38.685000200Z"
    },
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_keras_history import plot_history\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e77dd1c053a621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:24:20.258279800Z",
     "start_time": "2023-09-05T16:24:20.201399100Z"
    },
    "id": "18e77dd1c053a621"
   },
   "outputs": [],
   "source": [
    "with open('english-corpus.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "english = pd.DataFrame({'English-Sentences': lines})\n",
    "english['English-Sentences'] = english['English-Sentences'].str.rstrip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996f23c39284b569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:24:20.833141400Z",
     "start_time": "2023-09-05T16:24:20.751613300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "996f23c39284b569",
    "outputId": "995eec40-7bdc-4681-ad93-852603e41180"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English-Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is zain your nephew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i wish youd trust me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did he touch you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>its part of life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zain isnt ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>above all be patient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i learned it from him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>why am i doing this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i made a bad decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>zain wont care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       English-Sentences\n",
       "0    is zain your nephew\n",
       "1   i wish youd trust me\n",
       "2       did he touch you\n",
       "3       its part of life\n",
       "4         zain isnt ugly\n",
       "5   above all be patient\n",
       "6  i learned it from him\n",
       "7    why am i doing this\n",
       "8  i made a bad decision\n",
       "9         zain wont care"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff50740d5157eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:24:21.661110400Z",
     "start_time": "2023-09-05T16:24:21.620126100Z"
    },
    "id": "bff50740d5157eb4"
   },
   "outputs": [],
   "source": [
    "with open('urdu-corpus.txt', 'r',encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "urdu = pd.DataFrame({'Urdu-Sentences': lines})\n",
    "urdu['Urdu-Sentences'] = urdu['Urdu-Sentences'].str.rstrip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53a8a67dee82712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:24:22.263014600Z",
     "start_time": "2023-09-05T16:24:22.235877100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "c53a8a67dee82712",
    "outputId": "d65992f2-ac89-4185-845a-f88fa0ba1e09"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Urdu-Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>زین تمہارا بھتیجا ہے۔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>کاش تم مجھ پر بھروسہ کرتے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کیا اس نے آپ کو چھوا؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اس کی زندگی کا حصہ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>زین بدصورت نہیں ہے۔</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Urdu-Sentences\n",
       "0      زین تمہارا بھتیجا ہے۔\n",
       "1  کاش تم مجھ پر بھروسہ کرتے\n",
       "2      کیا اس نے آپ کو چھوا؟\n",
       "3         اس کی زندگی کا حصہ\n",
       "4        زین بدصورت نہیں ہے۔"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urdu.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b5b19567ec74e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:24:23.058401200Z",
     "start_time": "2023-09-05T16:24:23.031581Z"
    },
    "id": "8b5b19567ec74e2a"
   },
   "outputs": [],
   "source": [
    "data=pd.concat([english,urdu],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0d5f1810600f30b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:24:23.842381400Z",
     "start_time": "2023-09-05T16:24:23.778727600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "c0d5f1810600f30b",
    "outputId": "7c2f6a58-d28b-466f-dc84-b8323e6577bc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English-Sentences</th>\n",
       "      <th>Urdu-Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is zain your nephew</td>\n",
       "      <td>زین تمہارا بھتیجا ہے۔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i wish youd trust me</td>\n",
       "      <td>کاش تم مجھ پر بھروسہ کرتے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did he touch you</td>\n",
       "      <td>کیا اس نے آپ کو چھوا؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>its part of life</td>\n",
       "      <td>اس کی زندگی کا حصہ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zain isnt ugly</td>\n",
       "      <td>زین بدصورت نہیں ہے۔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>above all be patient</td>\n",
       "      <td>سب سے بڑھ کر صبر کرو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i learned it from him</td>\n",
       "      <td>میں نے اسے اس سے سیکھا۔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>why am i doing this</td>\n",
       "      <td>میں یہ کیوں کر رہا ہوں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i made a bad decision</td>\n",
       "      <td>میں نے ایک برا فیصلہ کیا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>zain wont care</td>\n",
       "      <td>زین پرواہ نہیں کرے گا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>zain was hesitant</td>\n",
       "      <td>زین ہچکچا رہا تھا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i borrowed zains car</td>\n",
       "      <td>میں نے زین گاڑی ادھار لی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>why are you out here</td>\n",
       "      <td>تم یہاں کیوں باہر ہو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>he is just a liar</td>\n",
       "      <td>وہ صرف جھوٹا ہے۔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>were almost done</td>\n",
       "      <td>تقریباً ہو گیا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what is happening</td>\n",
       "      <td>کیا ہو رہا ہے آج کل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zain betrayed us</td>\n",
       "      <td>زین نے ہمیں دھوکہ دیا۔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>zain goofed</td>\n",
       "      <td>زین بیوقوف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>everythings fine</td>\n",
       "      <td>سب ٹھیک ہے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>is it for here</td>\n",
       "      <td>کیا یہ یہاں کے لیے ہے؟</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        English-Sentences             Urdu-Sentences\n",
       "0     is zain your nephew      زین تمہارا بھتیجا ہے۔\n",
       "1    i wish youd trust me  کاش تم مجھ پر بھروسہ کرتے\n",
       "2        did he touch you      کیا اس نے آپ کو چھوا؟\n",
       "3        its part of life         اس کی زندگی کا حصہ\n",
       "4          zain isnt ugly        زین بدصورت نہیں ہے۔\n",
       "5    above all be patient       سب سے بڑھ کر صبر کرو\n",
       "6   i learned it from him    میں نے اسے اس سے سیکھا۔\n",
       "7     why am i doing this     میں یہ کیوں کر رہا ہوں\n",
       "8   i made a bad decision   میں نے ایک برا فیصلہ کیا\n",
       "9          zain wont care      زین پرواہ نہیں کرے گا\n",
       "10      zain was hesitant          زین ہچکچا رہا تھا\n",
       "11   i borrowed zains car   میں نے زین گاڑی ادھار لی\n",
       "12   why are you out here       تم یہاں کیوں باہر ہو\n",
       "13      he is just a liar           وہ صرف جھوٹا ہے۔\n",
       "14       were almost done             تقریباً ہو گیا\n",
       "15      what is happening        کیا ہو رہا ہے آج کل\n",
       "16       zain betrayed us     زین نے ہمیں دھوکہ دیا۔\n",
       "17            zain goofed                 زین بیوقوف\n",
       "18       everythings fine                 سب ٹھیک ہے\n",
       "19         is it for here     کیا یہ یہاں کے لیے ہے؟"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PERFORMING WORD COUNT AND KEEPING ONLY THOSE HAVING WORDS LESS THAN 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for word count\n",
    "def word_count (txt):\n",
    "    return len(txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['English-Words']=data['English-Sentences'].apply(lambda x: word_count(x))\n",
    "data['Urdu-Words']=data['Urdu-Sentences'].apply(lambda x: word_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24525, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English-Sentences</th>\n",
       "      <th>Urdu-Sentences</th>\n",
       "      <th>English-Words</th>\n",
       "      <th>Urdu-Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is zain your nephew</td>\n",
       "      <td>زین تمہارا بھتیجا ہے۔</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i wish youd trust me</td>\n",
       "      <td>کاش تم مجھ پر بھروسہ کرتے</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did he touch you</td>\n",
       "      <td>کیا اس نے آپ کو چھوا؟</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>its part of life</td>\n",
       "      <td>اس کی زندگی کا حصہ</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zain isnt ugly</td>\n",
       "      <td>زین بدصورت نہیں ہے۔</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      English-Sentences             Urdu-Sentences  English-Words  Urdu-Words\n",
       "0   is zain your nephew      زین تمہارا بھتیجا ہے۔              4           4\n",
       "1  i wish youd trust me  کاش تم مجھ پر بھروسہ کرتے              5           6\n",
       "2      did he touch you      کیا اس نے آپ کو چھوا؟              4           6\n",
       "3      its part of life         اس کی زندگی کا حصہ              4           5\n",
       "4        zain isnt ugly        زین بدصورت نہیں ہے۔              3           4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = data[(data['English-Words'] < 8) & (data['Urdu-Words'] < 8)]\n",
    "filtered_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22969, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPLITTING THE DATA INTO TRAIN AND TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(filtered_data,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADDING END OF LINE IN MY TRAIN TEST DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_EOL(lines):\n",
    "    text=[]\n",
    "    for line in lines:\n",
    "        text.append('<start> ' +line+ ' <end>')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDING EOL TO TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train=adding_EOL(list(train['English-Sentences']))\n",
    "urdu_train=adding_EOL(list(train['Urdu-Sentences']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDING EOL TO TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_test=adding_EOL(list(test['English-Sentences']))\n",
    "urdu_test=adding_EOL(list(test['Urdu-Sentences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOKENIZING THE TRAIN DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5237951bffeb79b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:24:25.256071500Z",
     "start_time": "2023-09-05T16:24:25.226362700Z"
    },
    "id": "5237951bffeb79b7"
   },
   "outputs": [],
   "source": [
    "def Tokenize_fn(X):\n",
    "    tokenizer=Tokenizer(filters='',lower=False)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    vocab_size=len(tokenizer.word_index)+1\n",
    "    return tokenizer.texts_to_sequences(X),vocab_size,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2acf6a33e4299944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:24:30.381634900Z",
     "start_time": "2023-09-05T16:24:30.333943700Z"
    },
    "id": "2acf6a33e4299944"
   },
   "outputs": [],
   "source": [
    "def pad_fn(X,length=None):\n",
    "    return pad_sequences(X, maxlen = length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZING ENGLISH TRAIN\n",
    "english_train_tokenized,num_ip_tokens,english_train_tok=Tokenize_fn(english_train)\n",
    "english_train=pad_fn(english_train_tokenized)\n",
    "# TOKENIZING URDU TRAIN\n",
    "urdu_train_tokenized,num_op_tokens,urdu_train_tok=Tokenize_fn(urdu_train)\n",
    "urdu_train=pad_fn(urdu_train_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_ip=english_train.shape[1]\n",
    "max_len_op=urdu_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4900"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ip_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOKENIZING TESTING DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZING ENGLISH TRAIN\n",
    "english_test_tokenized,num_ip_tokens_test,english_test_tok=Tokenize_fn(english_test)\n",
    "english_test=pad_fn(english_test_tokenized)\n",
    "# TOKENIZING URDU TRAIN\n",
    "urdu_test_tokenized,num_op_tokens_test,urdu_test_tok=Tokenize_fn(urdu_test)\n",
    "urdu_test=pad_fn(urdu_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5125cd1159171a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T16:38:34.606450100Z",
     "start_time": "2023-09-05T16:38:34.562608100Z"
    },
    "id": "5125cd1159171a23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2484"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ip_tokens_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOING SOME ENCODER DECODER DATA STUFF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d9c332a3628f6ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T17:22:36.886354100Z",
     "start_time": "2023-09-05T17:22:36.886354100Z"
    },
    "id": "7d9c332a3628f6ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE DECODER INPUT DATA IS  (18375, 8)\n",
      "THE DECODER TARGET DATA IS  (18375, 8)\n",
      "THE ENCODER INPUT DATA IS  (18375, 9)\n"
     ]
    }
   ],
   "source": [
    "# For decoder input we don't need the last word as it is only for prediction (teacher forcing)\n",
    "decoder_input_data=urdu_train[:,:-1]\n",
    "# For decoder output we are one step ahead of input (teacher forcing)\n",
    "decoder_target_data=urdu_train[:,1:]\n",
    "# Encoder input data\n",
    "encoder_input_data=english_train\n",
    "\n",
    "print(\"THE DECODER INPUT DATA IS \",decoder_input_data.shape)\n",
    "print(\"THE DECODER TARGET DATA IS \",decoder_target_data.shape)\n",
    "print(\"THE ENCODER INPUT DATA IS \",encoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE DECODER INPUT DATA IS  (4594, 8)\n",
      "THE DECODER TARGET DATA IS  (4594, 8)\n",
      "THE ENCODER INPUT DATA IS  (4594, 9)\n"
     ]
    }
   ],
   "source": [
    " # For testing Data\n",
    "test_decoder_input_data=urdu_test[:,:-1]\n",
    "test_decoder_target_data=urdu_test[:,1:]\n",
    "test_encoder_input_data=english_test\n",
    "\n",
    "print(\"THE DECODER INPUT DATA IS \",test_decoder_input_data.shape)\n",
    "print(\"THE DECODER TARGET DATA IS \",test_decoder_target_data.shape)\n",
    "print(\"THE ENCODER INPUT DATA IS \",test_encoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    3,   57,    7, 1927,    2,    0,    0,    0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE VOCABULARY SIZE FOR ENGLISH IS  4900\n",
      "THE VOCABULARY SIZE FOR URDU IS  5077\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens=num_ip_tokens\n",
    "num_decoder_tokens=num_op_tokens\n",
    "\n",
    "print(\"THE VOCABULARY SIZE FOR ENGLISH IS \", num_encoder_tokens)\n",
    "print(\"THE VOCABULARY SIZE FOR URDU IS \", num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADDIN MODEL HERE SEQUENCE TO SEQUENCE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for hidden units and for Embedding layer\n",
    "latent_dim=300\n",
    "\n",
    "#===========ENCODER-PART=================\n",
    "encoder_inputs=tf.keras.layers.Input(shape=(encoder_input_data.shape[1]),name='ENCODER-INPUT-LAYER')\n",
    "x=tf.keras.layers.Embedding(num_encoder_tokens,latent_dim, mask_zero=False ,name='EMBEDDING-LAYER-ENCODER')(encoder_inputs)\n",
    "x=tf.keras.layers.BatchNormalization(name='ENCODER-NORMALIZATION')(x)\n",
    "_,state_h=tf.keras.layers.GRU(latent_dim,return_state=True, name='ENCODER-GRU-LAYER')(x)\n",
    "encoder_model=tf.keras.Model(inputs=encoder_inputs,outputs=state_h, name='ENCODER-MODEL')\n",
    "seq2seq_encoder_out=encoder_model(encoder_inputs)\n",
    "#==========DECODER-PART==================\n",
    "decoder_inputs=tf.keras.layers.Input(shape=(None,),name='DECODER-INPUT-LAYER')\n",
    "y=tf.keras.layers.Embedding(num_decoder_tokens,latent_dim,name='DECODER-EMBEDDING-LAYER')(decoder_inputs)\n",
    "y=tf.keras.layers.BatchNormalization(name='DECODER-NORMALIZATION-1')(y)\n",
    "decoder_gru=tf.keras.layers.GRU(latent_dim,return_state=True,name='DECODER-GRU',return_sequences=True)\n",
    "decoder_gru_output,_=decoder_gru(y,initial_state=seq2seq_encoder_out)\n",
    "x=tf.keras.layers.BatchNormalization(name='DECODER-NORMALIZATION-2')(decoder_gru_output)\n",
    "decoder_dense=tf.keras.layers.Dense(num_decoder_tokens,activation='softmax',name='FINAL-OUTPUT-LAYER')\n",
    "decoder_outputs=decoder_dense(x)\n",
    "#============SEQUENCE TO SEQUENCE MODEL======\n",
    "seq2seq_model=tf.keras.Model(inputs=[encoder_inputs,decoder_inputs],outputs=decoder_outputs)\n",
    "seq2seq_model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " DECODER-INPUT-LAYER (InputLaye  [(None, None)]      0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " DECODER-EMBEDDING-LAYER (Embed  (None, None, 300)   1523100     ['DECODER-INPUT-LAYER[0][0]']    \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " ENCODER-INPUT-LAYER (InputLaye  [(None, 9)]         0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " DECODER-NORMALIZATION-1 (Batch  (None, None, 300)   1200        ['DECODER-EMBEDDING-LAYER[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " ENCODER-MODEL (Functional)     (None, 300)          2013000     ['ENCODER-INPUT-LAYER[0][0]']    \n",
      "                                                                                                  \n",
      " DECODER-GRU (GRU)              [(None, None, 300),  541800      ['DECODER-NORMALIZATION-1[0][0]',\n",
      "                                 (None, 300)]                     'ENCODER-MODEL[0][0]']          \n",
      "                                                                                                  \n",
      " DECODER-NORMALIZATION-2 (Batch  (None, None, 300)   1200        ['DECODER-GRU[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " FINAL-OUTPUT-LAYER (Dense)     (None, None, 5077)   1528177     ['DECODER-NORMALIZATION-2[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,608,477\n",
      "Trainable params: 5,606,677\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                               patience=3,  \n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "14/14 [==============================] - 32s 2s/step - loss: 6.1698 - val_loss: 8.3497\n",
      "Epoch 2/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 3.7516 - val_loss: 8.1405\n",
      "Epoch 3/70\n",
      "14/14 [==============================] - 24s 2s/step - loss: 2.9975 - val_loss: 8.1379\n",
      "Epoch 4/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 2.5628 - val_loss: 8.1102\n",
      "Epoch 5/70\n",
      "14/14 [==============================] - 26s 2s/step - loss: 2.1807 - val_loss: 8.0642\n",
      "Epoch 6/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 1.8277 - val_loss: 8.0132\n",
      "Epoch 7/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 1.5047 - val_loss: 7.9896\n",
      "Epoch 8/70\n",
      "14/14 [==============================] - 26s 2s/step - loss: 1.2172 - val_loss: 7.9516\n",
      "Epoch 9/70\n",
      "14/14 [==============================] - 26s 2s/step - loss: 0.9561 - val_loss: 7.9142\n",
      "Epoch 10/70\n",
      "14/14 [==============================] - 27s 2s/step - loss: 0.7458 - val_loss: 7.8797\n",
      "Epoch 11/70\n",
      "14/14 [==============================] - 26s 2s/step - loss: 0.5759 - val_loss: 7.8370\n",
      "Epoch 12/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.4353 - val_loss: 7.7774\n",
      "Epoch 13/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.3306 - val_loss: 7.7121\n",
      "Epoch 14/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.2595 - val_loss: 7.6260\n",
      "Epoch 15/70\n",
      "14/14 [==============================] - 26s 2s/step - loss: 0.2004 - val_loss: 7.5340\n",
      "Epoch 16/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.1577 - val_loss: 7.4343\n",
      "Epoch 17/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.1265 - val_loss: 7.2939\n",
      "Epoch 18/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.1032 - val_loss: 7.1497\n",
      "Epoch 19/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.0853 - val_loss: 7.0038\n",
      "Epoch 20/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.0714 - val_loss: 6.8460\n",
      "Epoch 21/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.0604 - val_loss: 6.6526\n",
      "Epoch 22/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.0517 - val_loss: 6.4295\n",
      "Epoch 23/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.0449 - val_loss: 6.1863\n",
      "Epoch 24/70\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.0384 - val_loss: 5.8933\n",
      "Epoch 25/70\n",
      "14/14 [==============================] - 26s 2s/step - loss: 0.0337 - val_loss: 5.6213\n",
      "Epoch 26/70\n",
      "14/14 [==============================] - 2127s 163s/step - loss: 0.0297 - val_loss: 5.3119\n",
      "Epoch 27/70\n",
      "14/14 [==============================] - 23s 2s/step - loss: 0.0266 - val_loss: 4.9777\n",
      "Epoch 28/70\n",
      "14/14 [==============================] - 27s 2s/step - loss: 0.0238 - val_loss: 4.7074\n",
      "Epoch 29/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0217 - val_loss: 4.3758\n",
      "Epoch 30/70\n",
      "14/14 [==============================] - 21s 2s/step - loss: 0.0201 - val_loss: 4.0421\n",
      "Epoch 31/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0189 - val_loss: 3.7169\n",
      "Epoch 32/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0179 - val_loss: 3.4268\n",
      "Epoch 33/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0169 - val_loss: 3.1420\n",
      "Epoch 34/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0162 - val_loss: 2.9723\n",
      "Epoch 35/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0151 - val_loss: 2.6975\n",
      "Epoch 36/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0148 - val_loss: 2.5285\n",
      "Epoch 37/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0149 - val_loss: 2.3078\n",
      "Epoch 38/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0149 - val_loss: 2.1954\n",
      "Epoch 39/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0146 - val_loss: 2.0875\n",
      "Epoch 40/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0141 - val_loss: 1.9742\n",
      "Epoch 41/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0135 - val_loss: 1.8983\n",
      "Epoch 42/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0129 - val_loss: 1.8275\n",
      "Epoch 43/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0122 - val_loss: 1.7395\n",
      "Epoch 44/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0111 - val_loss: 1.6839\n",
      "Epoch 45/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0104 - val_loss: 1.6688\n",
      "Epoch 46/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0095 - val_loss: 1.6426\n",
      "Epoch 47/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0091 - val_loss: 1.6166\n",
      "Epoch 48/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0083 - val_loss: 1.6081\n",
      "Epoch 49/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0075 - val_loss: 1.6130\n",
      "Epoch 50/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0071 - val_loss: 1.6114\n",
      "Epoch 51/70\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.0070 - val_loss: 1.6181\n"
     ]
    }
   ],
   "source": [
    "history=seq2seq_model.fit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size=1200,epochs=70,\n",
    "                          validation_split=0.1,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq_model.save_weights(\"SEQUENCE-TO-SEQUENCE-MODEL.h5\")\n",
    "seq2seq_model.load_weights(\"SEQUENCE-TO-SEQUENCE-MODEL.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 6s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "pred=seq2seq_model.predict([test_encoder_input_data,test_decoder_input_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4594, 8, 5077)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
